{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import re                  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vlad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1*** if the tweet is describing a real disaster, and ***0*** otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/real_train.csv').set_index('id')\n",
    "test = pd.read_csv('../../data/real_test.csv').set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "train = train.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test])\n",
    "df = df.drop(['location', 'keyword'], axis=1)\n",
    "split = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "\n",
    "df['text']=df['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "df['text']=df['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "df['text']=df['text'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['text']=df['text'].apply(lambda x : stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_df` is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    ">`max_df` = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    ">`max_df` = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "\n",
    "The default `max_df` is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "`min_df` is used for removing terms that appear too infrequently. For example:\n",
    "\n",
    ">`min_df` = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    ">`min_df` = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "\n",
    "The default `min_df` is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='char',\n",
    "                            stop_words='english',\n",
    "                            ngram_range=(1, 6),\n",
    "                            min_df=10, max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test = df_title[:split], df_title[split:]\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeClassifier(alpha=1, random_state=666).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (train) 0.943\n",
      "F1 score (holdout) 0.736\n"
     ]
    }
   ],
   "source": [
    "print('F1 score (train) %.3f' % f1_score(y_train, clf.predict(X_train)))\n",
    "print('F1 score (holdout) %.3f' % f1_score(y_holdout, clf.predict(X_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaned data\n",
    "- F1 score (train) 0.917\n",
    "- F1 score (holdout) 0.751\n",
    "- TfidfVectorizer(analyzer='char',stop_words='english', ngram_range=(1, 4), min_df=3, max_df=0.3)\n",
    "- Ridge, alpha=1.5\n",
    "- Results:\n",
    "  1. F1 score (train) 0.917\n",
    "  2. F1 score (holdout) 0.751\n",
    "  3. KAggle 0.79957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_range = [(1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
    "# min_df = [5, 10, 20]\n",
    "# max_df = [0.1, 0.2, 0.3, 0.4]\n",
    "# f1_results = {}\n",
    "# for i in tqdm(n_range):\n",
    "#     for j in min_df:\n",
    "#         for k in max_df:\n",
    "#             test_df = df\n",
    "#             vectorizer = TfidfVectorizer(analyzer='char',\n",
    "#                             stop_words='english',\n",
    "#                             ngram_range=i,\n",
    "#                             min_df=j,\n",
    "#                             max_df=k)\n",
    "#             df_title  = vectorizer.fit_transform(test_df['text'])\n",
    "#             X, X_test = df_title[:split], df_title[split:]\n",
    "#             X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, train_size=0.7, random_state=666)\n",
    "#             clf = RidgeClassifier(random_state=666).fit(X_train, y_train)\n",
    "#             f1_results['n_range=' + str(i) + \n",
    "#                        ' min_df=' + str(j) + \n",
    "#                        ' max_df=' + str(k)] = f1_score(y_holdout, clf.predict(X_holdout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.from_dict(f1_results, orient='index').rename(columns={0: 'F1'}).sort_values(by='F1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_mod = TfidfVectorizer(analyzer='char',\n",
    "                            stop_words='english',\n",
    "                            ngram_range=(3, 4),\n",
    "                            min_df=5, max_df=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha': [0.1, 0.5, 1, 2, 5, 10, 15, 20, 30]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = vectorizer_mod.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test = df_title[:split], df_title[split:]\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, train_size=0.7, random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(RidgeClassifier(random_state=666), param_grid=params, scoring='f1', cv=5).fit(X_train, y_train)\n",
    "best_clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (train) 0.896\n",
      "F1 score (holdout) 0.765\n"
     ]
    }
   ],
   "source": [
    "print('F1 score (train) %.3f' % f1_score(y_train, best_clf.predict(X_train)))\n",
    "print('F1 score (holdout) %.3f' % f1_score(y_holdout, best_clf.predict(X_holdout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pred, index=test.index, columns=['target']).to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TfidfVectorizer(analyzer='char',stop_words='english', ngram_range=(3, 4),minn_df=5, max_df=0.4)\n",
    "- RidgeClassifier\n",
    "- {'alpha': 2}\n",
    "- F1 score (train) 0.896\n",
    "- F1 score (holdout) 0.765\n",
    "- Kaggle 0.80570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
