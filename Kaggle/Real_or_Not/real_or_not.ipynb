{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import re     \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1*** if the tweet is describing a real disaster, and ***0*** otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/real_train.csv', encoding='utf-8')\n",
    "test = pd.read_csv('../../data/real_test.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(id             0\n",
       " keyword       61\n",
       " location    2533\n",
       " text           0\n",
       " target         0\n",
       " dtype: int64,\n",
       " id             0\n",
       " keyword       26\n",
       " location    1105\n",
       " text           0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum(), test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes, test_indexes = train['id'], test['id']\n",
    "train = train.drop(['id'], axis=1)\n",
    "test = test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "train['location'] = train['location'].fillna('No_location')\n",
    "test['location'] = test['location'].fillna('No_location')\n",
    "train['keyword'] = train['keyword'].fillna('No_keyword')\n",
    "test['keyword'] = test['keyword'].fillna('No_keyword')\n",
    "train = train.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = train.shape[0]\n",
    "df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def decontracted(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    #also delete @ and #\n",
    "    text = re.sub(r\"[@+#+]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "    return text\n",
    "\n",
    "#stemmer = SnowballStemmer('english')\n",
    "#stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function takes as input dataset and columns to process\n",
    "def clean_data(df, *variables):\n",
    "    for variable in variables:\n",
    "        df[variable] = df[variable].apply(lambda x : remove_URL(x))\n",
    "        df[variable] = df[variable].apply(lambda x : remove_emoji(x))\n",
    "        df[variable] = df[variable].apply(lambda x : decontracted(x))\n",
    "        df[variable] = df[variable].apply(lambda x : remove_punct(x))\n",
    "        df[variable] = df[variable].apply(lambda x : remove_stopwords(x))\n",
    "        #df[variable] = df[variable].apply(lambda x : stemmer.stem(x))\n",
    "        df[variable] = df[variable].apply(lambda x : lemmatizer.lemmatize(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nokeyword</td>\n",
       "      <td>Nolocation</td>\n",
       "      <td>Our Deeds Reason earthquake May ALLAH Forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nokeyword</td>\n",
       "      <td>Nolocation</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nokeyword</td>\n",
       "      <td>Nolocation</td>\n",
       "      <td>All residents asked ishelter place notified of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nokeyword</td>\n",
       "      <td>Nolocation</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nokeyword</td>\n",
       "      <td>Nolocation</td>\n",
       "      <td>Just got sent photo Ruby Alaska smoke wildfire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>Nokeyword</td>\n",
       "      <td>Nolocation</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>Nokeyword</td>\n",
       "      <td>Nolocation</td>\n",
       "      <td>Storm RI worse last hurricane My cityamp3other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>Nokeyword</td>\n",
       "      <td>Nolocation</td>\n",
       "      <td>Green Line derailment Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>Nokeyword</td>\n",
       "      <td>Nolocation</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook HWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>Nokeyword</td>\n",
       "      <td>Nolocation</td>\n",
       "      <td>CityofCalgary activated Municipal Emergency Pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        keyword    location                                               text\n",
       "0     Nokeyword  Nolocation   Our Deeds Reason earthquake May ALLAH Forgive us\n",
       "1     Nokeyword  Nolocation              Forest fire near La Ronge Sask Canada\n",
       "2     Nokeyword  Nolocation  All residents asked ishelter place notified of...\n",
       "3     Nokeyword  Nolocation  13000 people receive wildfires evacuation orde...\n",
       "4     Nokeyword  Nolocation  Just got sent photo Ruby Alaska smoke wildfire...\n",
       "...         ...         ...                                                ...\n",
       "3258  Nokeyword  Nolocation  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n",
       "3259  Nokeyword  Nolocation  Storm RI worse last hurricane My cityamp3other...\n",
       "3260  Nokeyword  Nolocation                      Green Line derailment Chicago\n",
       "3261  Nokeyword  Nolocation           MEG issues Hazardous Weather Outlook HWO\n",
       "3262  Nokeyword  Nolocation  CityofCalgary activated Municipal Emergency Pl...\n",
       "\n",
       "[10876 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(df, 'keyword', 'location', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['location'] + \" \" + df['text']\n",
    "df_keyword = df.drop(['location', 'text'], axis=1)\n",
    "df = df.drop(['location', 'keyword'], axis=1)\n",
    "train = df[:idx]\n",
    "test = df[idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_df` is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    ">`max_df` = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    ">`max_df` = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "\n",
    "The default `max_df` is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "`min_df` is used for removing terms that appear too infrequently. For example:\n",
    "\n",
    ">`min_df` = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    ">`min_df` = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "\n",
    "The default `min_df` is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DF = 0.9\n",
    "MIN_COUNT = 5\n",
    "NGRAMS = (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r'[a-z]+|-?\\d*[-.,]?\\d+|\\S')\n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=2):\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.9, min_df=5, ngram_range=(1, 3),\n",
       "                tokenizer=<function tokenize_text_simple_regex at 0x7f4e7a938560>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = TfidfVectorizer(tokenizer=tokenize_text_simple_regex,\n",
    "                            min_df=MIN_COUNT, max_df=MAX_DF,\n",
    "                            ngram_range = NGRAMS)\n",
    "vector.fit(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nolocation', 1105),\n",
       " ('the', 311),\n",
       " ('new', 201),\n",
       " ('like', 146),\n",
       " ('amp', 136),\n",
       " ('usa', 122),\n",
       " ('via', 110),\n",
       " ('fire', 108),\n",
       " ('get', 108),\n",
       " ('would', 98)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(test['text'], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataset into sparse matrix\n",
    "def vectorize_data(df, vectorizer):\n",
    "    return vectorizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vect = vectorize_data(train['text'], vector)\n",
    "test_vect = vectorize_data(test['text'], vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<7613x5062 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 83035 stored elements in Compressed Sparse Row format>,\n",
       " <3263x5062 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 33941 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vect, test_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vector.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tf_idf_words(response, top_n=2):\n",
    "    sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n",
    "    return feature_names[response.indices[sorted_nzs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nolocation', 'stay', 'make', 'meltdown', 'attack'], dtype='<U41')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_tf_idf_words(train_vect, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword = df_keyword.to_dict(orient='records')\n",
    "dv_X = DictVectorizer(sparse=True)\n",
    "df_encoded = dv_X.fit_transform(df_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10876x214 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10876 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = df_encoded[:idx]\n",
    "test_encoded = df_encoded[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack([train_vect, train_encoded])\n",
    "X_test = hstack([test_vect, test_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<7613x5276 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 90648 stored elements in COOrdinate format>,\n",
       " <3263x5276 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 37204 stored elements in COOrdinate format>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, \n",
    "                                            train_size=0.7, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=SEED).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (train) 0.828\n",
      "F1 score (holdout) 0.733\n"
     ]
    }
   ],
   "source": [
    "print('F1 score (train) %.3f' % f1_score(y_train, clf1.predict(X_train)))\n",
    "print('F1 score (holdout) %.3f' % f1_score(y_holdout, clf1.predict(X_holdout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf1.predict(X_test)\n",
    "pd.DataFrame(pred, index=test_indexes, columns=['target']).to_csv('submisson.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KAGGLE 0.79037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (train) 0.896\n",
      "F1 score (holdout) 0.735\n"
     ]
    }
   ],
   "source": [
    "clf2 = RidgeClassifier(random_state=SEED).fit(X_train, y_train)\n",
    "print('F1 score (train) %.3f' % f1_score(y_train, clf2.predict(X_train)))\n",
    "print('F1 score (holdout) %.3f' % f1_score(y_holdout, clf2.predict(X_holdout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 2}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'alpha' : [0.001, 0.01, 0.1, 0.5, 1, 2, 3, 5, 7, 10]}\n",
    "grid = GridSearchCV(RidgeClassifier(random_state=SEED), param_grid=params, \n",
    "                    scoring='f1', cv=5).fit(X_train, y_train)\n",
    "best_clf = grid.best_estimator_\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (train) 0.867\n",
      "F1 score (holdout) 0.740\n"
     ]
    }
   ],
   "source": [
    "print('F1 score (train) %.3f' % f1_score(y_train, best_clf.predict(X_train)))\n",
    "print('F1 score (holdout) %.3f' % f1_score(y_holdout, best_clf.predict(X_holdout)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 score (train) 0.858 and F1 score (holdout) 0.743**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_clf.predict(X_test)\n",
    "pd.DataFrame(pred, index=test_indexes.index, columns=['target']).to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
