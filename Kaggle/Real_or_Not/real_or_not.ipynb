{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import re     \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1*** if the tweet is describing a real disaster, and ***0*** otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/real_train.csv', encoding='utf-8').set_index('id')\n",
    "test = pd.read_csv('../../data/real_test.csv',encoding='utf-8').set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(keyword       61\n",
       " location    2533\n",
       " text           0\n",
       " target         0\n",
       " dtype: int64,\n",
       " keyword       26\n",
       " location    1105\n",
       " text           0\n",
       " dtype: int64)"
      ]
     },
     "metadata": {},
     "execution_count": 978
    }
   ],
   "source": [
    "train.isnull().sum(), test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = input_train['target']\n",
    "train['location'] = train['location'].fillna('No_location')\n",
    "test['location'] = test['location'].fillna('No_location')\n",
    "train['keyword'] = train['keyword'].fillna('No_keyword')\n",
    "test['keyword'] = test['keyword'].fillna('No_keyword')\n",
    "train = train.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = train.shape[0]\n",
    "df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def decontracted(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    #also delete @ and #\n",
    "    text = re.sub(r\"[@+#+]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "    return text\n",
    "\n",
    "#stemmer = SnowballStemmer('english')\n",
    "#stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function takes as input dataset and columns to process\n",
    "def clean_data(df, *variables):\n",
    "    for variable in variables:\n",
    "        df[variable] = df[variable].apply(lambda x : remove_URL(x))\n",
    "        df[variable] = df[variable].apply(lambda x : remove_emoji(x))\n",
    "        df[variable] = df[variable].apply(lambda x : decontracted(x))\n",
    "        df[variable] = df[variable].apply(lambda x : remove_punct(x))\n",
    "        df[variable] = df[variable].apply(lambda x : remove_stopwords(x))\n",
    "        #df[variable] = df[variable].apply(lambda x : stemmer.stem(x))\n",
    "        df[variable] = df[variable].apply(lambda x : lemmatizer.lemmatize(x))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         keyword    location  \\\n",
       "id                             \n",
       "1      Nokeyword  Nolocation   \n",
       "4      Nokeyword  Nolocation   \n",
       "5      Nokeyword  Nolocation   \n",
       "6      Nokeyword  Nolocation   \n",
       "7      Nokeyword  Nolocation   \n",
       "...          ...         ...   \n",
       "10861  Nokeyword  Nolocation   \n",
       "10865  Nokeyword  Nolocation   \n",
       "10868  Nokeyword  Nolocation   \n",
       "10874  Nokeyword  Nolocation   \n",
       "10875  Nokeyword  Nolocation   \n",
       "\n",
       "                                                    text  \n",
       "id                                                        \n",
       "1       Our Deeds Reason earthquake May ALLAH Forgive us  \n",
       "4                  Forest fire near La Ronge Sask Canada  \n",
       "5      All residents asked ishelter place notified of...  \n",
       "6      13000 people receive wildfires evacuation orde...  \n",
       "7      Just got sent photo Ruby Alaska smoke wildfire...  \n",
       "...                                                  ...  \n",
       "10861  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...  \n",
       "10865  Storm RI worse last hurricane My cityamp3other...  \n",
       "10868                      Green Line derailment Chicago  \n",
       "10874           MEG issues Hazardous Weather Outlook HWO  \n",
       "10875  CityofCalgary activated Municipal Emergency Pl...  \n",
       "\n",
       "[10876 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Nokeyword</td>\n      <td>Nolocation</td>\n      <td>Our Deeds Reason earthquake May ALLAH Forgive us</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Nokeyword</td>\n      <td>Nolocation</td>\n      <td>Forest fire near La Ronge Sask Canada</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Nokeyword</td>\n      <td>Nolocation</td>\n      <td>All residents asked ishelter place notified of...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Nokeyword</td>\n      <td>Nolocation</td>\n      <td>13000 people receive wildfires evacuation orde...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Nokeyword</td>\n      <td>Nolocation</td>\n      <td>Just got sent photo Ruby Alaska smoke wildfire...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10861</th>\n      <td>Nokeyword</td>\n      <td>Nolocation</td>\n      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n    </tr>\n    <tr>\n      <th>10865</th>\n      <td>Nokeyword</td>\n      <td>Nolocation</td>\n      <td>Storm RI worse last hurricane My cityamp3other...</td>\n    </tr>\n    <tr>\n      <th>10868</th>\n      <td>Nokeyword</td>\n      <td>Nolocation</td>\n      <td>Green Line derailment Chicago</td>\n    </tr>\n    <tr>\n      <th>10874</th>\n      <td>Nokeyword</td>\n      <td>Nolocation</td>\n      <td>MEG issues Hazardous Weather Outlook HWO</td>\n    </tr>\n    <tr>\n      <th>10875</th>\n      <td>Nokeyword</td>\n      <td>Nolocation</td>\n      <td>CityofCalgary activated Municipal Emergency Pl...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10876 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 983
    }
   ],
   "source": [
    "clean_data(df, 'keyword', 'location', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['location'] + \" \" + df['text']\n",
    "df_keyword = df.drop(['location', 'text'], axis=1)\n",
    "df = df.drop(['location', 'keyword'], axis=1)\n",
    "train = df[:idx]\n",
    "test = df[idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_df` is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n",
    "\n",
    ">`max_df` = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    ">`max_df` = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "\n",
    "The default `max_df` is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "`min_df` is used for removing terms that appear too infrequently. For example:\n",
    "\n",
    ">`min_df` = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    ">`min_df` = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "\n",
    "The default `min_df` is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DF = 0.8\n",
    "MIN_COUNT = 5\n",
    "NGRAMS = (3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r'[a-z]+|-?\\d*[-.,]?\\d+|\\S')\n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=2):\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', max_df=0.8, min_df=5, ngram_range=(3, 5))"
      ]
     },
     "metadata": {},
     "execution_count": 987
    }
   ],
   "source": [
    "vector = TfidfVectorizer(#tokenizer=tokenize_text_simple_regex,\n",
    "                            analyzer='char',\n",
    "                            min_df=MIN_COUNT, max_df=MAX_DF,\n",
    "                            ngram_range = NGRAMS)\n",
    "vector.fit(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataset into sparse matrix\n",
    "def vectorize_data(df, vectorizer):\n",
    "    return vectorizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vect = vectorize_data(train['text'], vector)\n",
    "test_vect = vectorize_data(test['text'], vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<7613x45139 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1465915 stored elements in Compressed Sparse Row format>,\n",
       " <3263x45139 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 622198 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "metadata": {},
     "execution_count": 990
    }
   ],
   "source": [
    "train_vect, test_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword = df_keyword.to_dict(orient='records')\n",
    "dv_X = DictVectorizer(sparse=True)\n",
    "df_encoded = dv_X.fit_transform(df_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<10876x214 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10876 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 992
    }
   ],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = df_encoded[:idx]\n",
    "test_encoded = df_encoded[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack([train_vect, train_encoded])\n",
    "X_test = hstack([test_vect, test_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<7613x45353 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1473528 stored elements in COOrdinate format>,\n",
       " <3263x45353 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 625461 stored elements in COOrdinate format>)"
      ]
     },
     "metadata": {},
     "execution_count": 995
    }
   ],
   "source": [
    "X, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, \n",
    "                                            train_size=0.7, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=SEED).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score (train) 0.845\nF1 score (holdout) 0.731\n"
     ]
    }
   ],
   "source": [
    "print('F1 score (train) %.3f' % f1_score(y_train, clf1.predict(X_train)))\n",
    "print('F1 score (holdout) %.3f' % f1_score(y_holdout, clf1.predict(X_holdout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf1.predict(X_test)\n",
    "pd.DataFrame(pred, index=test.index, columns=['target']).to_csv('submisson.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KAGGLE 0.79037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score (train) 0.950\nF1 score (holdout) 0.741\n"
     ]
    }
   ],
   "source": [
    "clf2 = RidgeClassifier(random_state=SEED).fit(X_train, y_train)\n",
    "print('F1 score (train) %.3f' % f1_score(y_train, clf2.predict(X_train)))\n",
    "print('F1 score (holdout) %.3f' % f1_score(y_holdout, clf2.predict(X_holdout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'alpha': 2}"
      ]
     },
     "metadata": {},
     "execution_count": 1005
    }
   ],
   "source": [
    "params = {'alpha' : [0.001, 0.01, 0.1, 0.5, 1, 2, 3, 5, 7, 10]}\n",
    "grid = GridSearchCV(RidgeClassifier(random_state=SEED), param_grid=params, \n",
    "                    scoring='f1', cv=5).fit(X_train, y_train)\n",
    "best_clf = grid.best_estimator_\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score (train) 0.900\nF1 score (holdout) 0.739\n"
     ]
    }
   ],
   "source": [
    "print('F1 score (train) %.3f' % f1_score(y_train, best_clf.predict(X_train)))\n",
    "print('F1 score (holdout) %.3f' % f1_score(y_holdout, best_clf.predict(X_holdout)))"
   ]
  },
  {
   "source": [
    "**F1 score (train) 0.858 and F1 score (holdout) 0.743**\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_clf.predict(X_test)\n",
    "pd.DataFrame(pred, index=test.index, columns=['target']).to_csv('submission_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}